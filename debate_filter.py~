import json
import os
from datetime import datetime
from local_llm import call_local_model

INPUT_FILE = "training_data.jsonl"
OUTPUT_FILE = "debated_training_data.jsonl"

MODEL_A = "mistral-instruct"
MODEL_B = "reasoner-v1"
JUDGE_MODEL = "llama3.1-8b-instruct-128k"



def model_score(model, instruction, response):
    prompt = f"""
Evaluate this instruction-response pair from 1 to 10:

Instruction:
{instruction}

Response:
{response}

Give only the number.
"""
    reply = call_local_model(prompt, model)
    digits = [int(c) for c in reply if c.isdigit()]
    return max(digits) if digits else 1

def judge_disagreement(instruction, response, score_a, score_b):
    prompt = f"""
Two models scored this instruction-response pair:
Instruction:
{instruction}

Response:
{response}

Model A score: {score_a}
Model B score: {score_b}

Should this example be included in a fine-tuning dataset? Justify your answer and give a final yes/no.
"""
    return call_local_model(prompt, JUDGE_MODEL)

def debate_filter(input_path, output_path, agree_threshold=6):
    print("[DebateFilter] Starting debate filter...")
    kept, skipped, debated = 0, 0, 0

    with open(input_path, "r") as infile, open(output_path, "w") as outfile:
        for line in infile:
            example = json.loads(line)
            instr, resp = example["instruction"], example["response"]

            score_a = model_score(MODEL_A, instr, resp)
            score_b = model_score(MODEL_B, instr, resp)

            if score_a >= agree_threshold and score_b >= agree_threshold:
                reason = "Both models agree it's good."
                final_score = (score_a + score_b) // 2
                result = "accepted"

            elif score_a < agree_threshold and score_b < agree_threshold:
                reason = "Both models agree it's garbage."
                result = "rejected"
                skipped += 1
                continue

            else:
                debated += 1
                verdict = judge_disagreement(instr, resp, score_a, score_b).lower()
                if "yes" in verdict:
                    reason = "Disagreement resolved by judge: keep."
                    final_score = max(score_a, score_b)
                    result = "accepted"
                else:
                    reason = "Disagreement resolved by judge: discard."
                    result = "rejected"
                    skipped += 1
                    continue

            example["score"] = final_score
            example["metadata"].update({
                "scored_at": datetime.now().isoformat(),
                "score_a": score_a,
                "score_b": score_b,
                "result": result,
                "reasoning": reason
            })
            outfile.write(json.dumps(example) + "\n")
            kept += 1

    print(f"[DebateFilter] Accepted: {kept}, Rejected: {skipped}, Debated: {debated}")

if __name__ == "__main__":
    if not os.path.exists(INPUT_FILE):
        print("[Error] Input file not found.")
    else:
        debate_filter(INPUT_FILE, OUTPUT_FILE)
